我将分两步实现该高性能 Warp RL 训练环境。

### 第一阶段：重构 Warp 物理环境 (`env_warp/aero_warp.py`)

为了满足 "2万飞机+2万导弹" 及 "全GPU流程" 的需求，需要将原本简单的运动学 demo 扩展为完整的游戏逻辑，并保证性能不下降到1000FPS以下。

1. **数据结构升级**:

   * 引入 `Fighter` 和 `Missile` 的 Warp Struct，管理位置、速度、朝向等状态。

   * 为 10,000 个环境分配连续的显存空间（Struct Array）。
2. **核心 Kernel 实现**:

   * `integrate_physics`: 统一处理飞机和导弹的运动学积分（2D平面），导弹和飞机的物理特性参考env\_numpy和config当中的数据。

   * `missile_logic`: 实现比例导引（PN）算法，以及“单通道火控”逻辑（导弹命中/脱靶后自动回收重置）。在火力通道设定下，已有飞行中的导弹时，若强制发射导弹，会导致正在飞行中的那颗导弹“自爆”（其实是同一个实体从载机上重新飞出来）。

   * `collision_check`: 高频判定导弹与飞机的距离，处理一击毙命的击杀逻辑（要注意）。导弹杀伤半径设定100米。

   * `get_observations`: 将敌我飞机与导弹的物理状态归一化并写入 Tensor 供网络输入。

   * `compute_rewards`: **关键**，在 Kernel 内直接计算奖励（如角度优势、敌人有导弹时保持安全距离优势、击杀奖励、导弹浪费惩罚、导弹接近/逼退敌机奖励、躲避敌导弹奖励），写入 Reward Tensor。
3. **环境接口改造**:

   * `step()` 函数将不再返回 CPU 数据，而是返回 GPU 上的 `obs_torch`, `reward_torch`, `done_torch`。

   * 实现 `reset_hard` 和 `reset_auto`（死后自动复活）。

   * 游戏为1V1 。支持使用规则型智能体作为对手，也可使用自身的历史版本作为对手。对手的参数不会被训练。用户可以通过在训练循环中暂停期间手动按V观察是否得到满意结果，然后按S保存，然后在重新启动训练脚本时使用更高级的对手和更新版本的奖励函数来让智能体学到新的行为。（基于人工观察的手动的课程式强化学习）

### 第二阶段：实现 PPO 训练主循环 (`train_warp.py`)

基于 "零拷贝" 原则，实现纯 GPU 的 PPO 训练循环。

1. **PPO 模块**:

   * 实现简单的 Actor-Critic 网络（全连接层）。

   * 实现 `RolloutStorage`，在 GPU 上预分配 `(num_steps, num_envs, ...)` 维度的 Tensor，用于记录 600 步的轨迹。
2. **训练主循环**:

   * **物理循环 (6000 步)**: 内部循环 10 次 Warp 物理步。

   * **决策循环 (600 步)**: 每 10 物理步提取一次 Obs，推理 Action，写入 Warp。

   * **PPO 更新**: 每 600 决策步（相当于游戏内10分钟，如果性能是1000次物理帧更新每现实秒，那么就是6秒现实时间），暂停环境，利用 GPU 上的轨迹数据进行梯度下降。
3. **交互与可视化**:

   * 实现键盘监听（非阻塞），响应 P/R/Q/V/S 指令。

   * **可视化模式**: 按 V 键后，提取单个环境的最近 600 步轨迹数据（需从 GPU 拷贝极少量数据到 CPU），使用 Pygame 及其轻量级绘图功能进行 10 倍速回放。

### 待确认事项

* 目前设计为 2D 物理（基于原代码 `vec2`），如需 3D 请指出。（用户确认不需要3D）

* 动作空间将设为 `MultiDiscrete([3, 2])`（舵量3档，开火2档）。（正确）

我将首先重写 `env_warp/aero_warp.py`，然后实现 `train_warp.py`。
