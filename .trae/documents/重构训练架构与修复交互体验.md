我将执行以下重构和修复计划：

### 1. 代码重构 (创建 `RL/` 模块)

将训练逻辑拆分为独立的模块，放在 `RL` 文件夹下：

* **`RL/default_model.py`**: 定义 `ActorCriticAgent` 神经网络结构。

* **`RL/ppo_v1.py`**: 封装 PPO 算法核心，包括 `RolloutStorage` 管理和 `update` 参数更新逻辑。

* **`RL/utils.py`**: 包含键盘监听线程类（修复输入无响应问题）和其他辅助工具。

### 2. 修复键盘响应

使用 **独立守护线程** 监听键盘输入，将按键存入线程安全的队列中，主循环在每一帧检查队列，从而彻底解决阻塞导致的无响应问题。

### 3. 可视化适配

编写 **适配器类**，将 Warp 的 GPU 数据（Tensor/Array）转换为 `visualization.Visualizer` 所需的对象格式（拥有 `.x`, `.y`, `.angle` 等属性），从而复用现有的可视化 API。

### 4. 终端显示优化

* **SPS 显示**: 将显示 "Per-Env SPS"（单环境吞吐量），即总步数除以环境数。

* **状态分阶段显示**: 使用动态刷新（`\r`）在同一行显示当前状态：

  * 物理阶段: `[Physics] Step X/600 | SPS: 1200.5 ...`

  * 更新阶段: `[Train] Epoch X/4 | Loss: 0.123 ...`

### 5. 文件结构变更

* 创建 `RL/` 目录。

* 更新 `train_warp.py` 以引用新模块。

* （可选）如果需要，我会调整 `env_warp` 中的代码以支持新的接口，但主要变动在 Python 层。

我将从创建 `RL` 目录和子模块开始。
