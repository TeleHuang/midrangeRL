=======================================================
传统软件工程的开发关键原则：
1、版本控制：feature branch，合并前需通过CI测试，写清楚为什么改
2、接口契约与文档：README+API spec
3、自动化测试：单元测试，集成测试，测试驱动开发与断言式测试
4、持续集成、持续交付
5、架构图：组件图、时序图、数据流图

个人开发小项目时利用AI写代码的工作流程：
一、讨论和定义需求，需求要写在项目文档里
二、生成代码并测试
三、两轮Code Review：
1、确认可运行后，根据输入输出关键词搜索程序文件内关键字段，利用“查看定义”等IDE代码定位工具，查看内部功能实现是否存在问题，并在查看时编写注释。
2、交给另一个AI评价。
四、发现问题时，注意要编写TODO和Issue，然后回到第一步，直到问题全部解决
五、审核通过后提交代码

=======================================================
midrangeRL v0.2.1
env_warp
agents
spacetime
train_warp.py
game_play.py
=======================================================
开发日志：1月：

项目初始时，并行物理运算使用的是tensor数据结构+大量小kernel，存在大量额外性能开销。

//12日，13日，性能不达标，我本应该明确定义好性能需求：预期200赫兹及以上的物理帧更新频率，20倍以上时间加速，仿真世界内10帧表示一秒。
（低帧模式下应该改用长方形扫掠区域法来判定命中，就是在垂直于弹目相对速度的方向上拉一根200米长、以导弹为中点的线段，在关键的命中判定帧当中计算飞机是否被这个长方形覆盖）

//14日，首次实现较为流畅的PPO参数更新。头12K步当中V Loss一直在上升，之后突然下降。不确定这是否有问题，还需要学习理论进行鉴别。

//15日，狠狠地学习了理论，看懂了PPO。在Actor-Critic架构当中，V的标准是会变的。
今天决定用离散动作组来代替连续的，因为这个确实省很多事。
今天通过Warp官方colab教程发现1000帧每现实秒的性能是可行的，可以预期100倍以上的时间加速倍率，我需要的结构长这样：
Warp (physics)
   ↓
Torch tensor (obs)
   ↓
Torch PPO
   ↓
Torch tensor (action)
   ↓
Warp (apply force)
完全不经过主机，不使用字典等普通Python数据类型，直接指定显存管道进行obs/action交互（warp原生支持torch to warp这样的转化）。
GPT说我现在用的Gymnasium是以CPU为核心的。也就是说，即使环境和神经网络都在GPU上，它们之间的互动也需要经过主机。
要是早知道这个事情，能省很多事。可惜我一开始没问出这个问题。现在这坨屎山可以推倒了。

//16日，准备开始实现上述架构，新建一个brunch。
实现了warp环境，并且跑通了训练程序，但是当前的训练程序还无法产生可用的强化学习结果。
大约下午五点，删除了env_gym，但还保留着依赖env_gym的train，准备让train_warp用warp来高性能地实现train的功能。
下午六点十分，课程学习效果看起来很有问题，阶段一之下双方居然都是agent，这明显不符合需求。
发现问题根源，可视化程序当中发生的事情和训练发生的事情不一样，可视化程序里面的双方是随机运动。
基础版warp环境开发成功，进入清理垃圾文件阶段。
训练失败，第一阶段课程没能取得该有的效果。

//17日，先清理一下垃圾文件，把上一阶段剩余的tensor物理运算程序这些失败品清除，让新的warp物理运算程序有个更干净的环境。暂时放弃GPU版时空图，这个稍后再讨论如何实现。
然后把train、tensor_spacetime删掉，注意与它们相关的依赖，比如spacetime_renderer的tensor模式，也要做相应的修改，将指向tensor版时空图的功能弃用。
准备优化一下可视化程序，增加拷贝帧间距，减少拷贝的帧数。
准备把Reward设计交给LLM试一下，看能不能突破瓶颈。
很可能还需要重新设计一下观测空间，更改为相对位置+本机目前朝向。
也有可能需要改进一下模型本身的结构。

//19日，准备完成17日未完成的目标。
发现实际FPS没有几万那么多，而是只有100甚至更少，和之前没区别。
那这说明代码的性能是不合格的，它远远没有达到我需要的帧率，要重构。
这次决定自己来，不用这个愚蠢的AI了。

//20日，需求定义：游戏内一秒～10帧物理更新与1帧网络决策，相对于现实100倍时间加速（1000SPS），神经网络100帧推理
一环境当中最多只能有一两枚导弹，双方各一颗，当导弹自毁后会重新回到它的飞机上。这样可以保证一次只发射一枚，节省弹药，并且节约算力。（“火力通道”设定，注意这一点与CPU版不同）
在火力通道设定下，已有飞行中的导弹时强制发射导弹会导致飞行中的导弹“自爆”（其实是同一个实体从载机上重新飞出来）。
=======================================================
未实现的IDEA
性能优化：改用Warp,使用Nsight生态分析
在Nsight Compute：
可执行文件：C:/anaconda3/envs/cleanrl/python.exe
工作文件夹：D:/Workspace/Python/PyBasicalPractice/games/midrangeR
命令行：train_warp.py --mode cold_start --num-envs 10000 --device cuda

好像启动前要根据需要在程序中打上断点/标记，这一点我还没确定。Night支持分析Python warp jit编译生成的程序。

在Nsight System：
命令行：C:/anaconda3/envs/cleanrl/python.exe train_warp.py --mode cold_start --num-envs 10000 --device cuda


好玩的想法：Step as chess，事件驱动，单次运算会直接一直更新到下一次事件发生瞬间。
飞机行为由离散可定时的直线飞行/满舵两种要素构成，具有超低决策频率，Agent只输出“左转，4.733秒”这样的信号。
环境考虑具备“悔棋”功能，游玩起来会非常好玩，并且悔棋功能在RL上也有想象空间。

RL改进：
1、ppo_descrete使用离散动作空间。（发现参数太少，效果不好）
2、ppo_seq，加大模型参数规模和结构复杂度，使用transformer架构，可能会使用相对低频的推理频率来节约算力。
3、ppo_world_model，模型是世界模型的操作者，对世界模型输入预测请求，然后世界模型返回预测来作为决策依据。

远期：
时空图v1.2：对agent创建的特定动作组下的导弹行为做出预测
时空图v2：多条决策路径并行预测
时空图v3：基于扩散模型的概率云时空图

mindrangeRL2：
三维空间物理运算+四维时空图辅助决策。
=======================================================
当前特性：

/Alpha/：时空图模式v1版本特性描述：

一、总体

二维的战场加上一个时间维会被显示为一个扁长方体，两个水平轴为战场空间轴，一个竖直轴为时间轴，当前时刻截面下方会显示20秒的未来。

查看该图像的视角俯角固定为30度左右，左右方向可以由玩家操作绕竖直轴旋转，两个玩家各有一个时空图。红方用“QE”按钮控制视图YAW，蓝方用“0-”按钮控制。
现在已有二维战场可视化方案，时空图要继承该方案的画风：玩家、导弹、轨迹的颜色，实体的符号和显示马赫与G值的悬浮HUD，图的中心跟随玩家飞机运动，时空图的立方体当中有3层在时间上等距的水平网格，间隔为10秒，将长方体划分为两层。最上面一层网格面显然就是现在，而下面两层分别是10秒后和20秒后。网格的后退表示地面相对飞机后退。

三维图形显示方式暂定为简单的透视投影，用Pygame渲染，不引入任何3D库。要注意，需要做出来适当的近大远小，不能用平行投影。

时空图是用来估计导弹实际射程的，这样玩家可以有依据地决定在什么时候开始调头。比如说假设300米每秒射出的导弹在飞行出20公里的位置减速到无威胁，那玩家可以根据时空图判定，自己可以前进到十五公里再掉头回退，仍然来得及在导弹追上自己前撤退到20公里位置。

二、特性

时空图主要针对两个内容进行预测：1、若此刻发射导弹，是否有可能在未来追上全力回转的对方。2、基于被追踪对象当前舵量对正在飞行的导弹未来行为进行预测。

时空图对agent同时展示进攻与防御相关的内容，既展示己方导弹对敌方的进攻情况，又展示敌方对己方的威胁情况。在游玩模式下，时空图只对玩家展示有关防御的内容，若需查看有关进攻的内容，则需看对方的屏幕。

对于A玩家，他会看到未来时空图存在一个以B玩家为顶点的威胁圆锥，这个圆锥用于表示B玩家如果在此刻发射导弹，未来这颗导弹可能到达的位置范围。圆锥只有B玩家前半球部分，在B玩家正前方的威胁圆锥母线是导弹沿着直线飞行时的典型飞行过程时空曲线，正侧方母线在空间轴缩小至原来一半，在单一时刻截面下，威胁锥是覆盖前半球的半个椭圆。当导弹物理特性已经确定，曲线的初始斜率和整体趋势显然仅由载机飞行速度决定，所以这一部分可以用打表法制作，在开始前就用单独的脚本计算好载机速度200到500范围内的全部的导弹直线飞行过程（最好把它们Plot出来让用户能肉眼查证其合理性），在实际运行过程中生成时空图时只需查表（注意与训练环境的相容性和性能）。该威胁锥并不需要Mesh填充为实面，只需要把几条母线，和时间截面上的椭圆线创建好即可，玩家可以根据这个网格自己脑补出圆锥的样子，AI只需读取其中几十个关键点作为输入的特征。

此外，玩家能看到自己的三条运动轨迹预测线，分别是向左/右全力脱离曲线（黄色线，可以开启或者关闭，蓝方按-，红方按R），和当前舵量预测曲线（低饱和度蓝/低饱和度红线，与历史轨迹线一致），这是用物理引擎现算的，每0.5秒计算一次，使用较大的时间微分步长。敌人发射导弹前，当时空图中整条全力掉头曲线处于敌人的威胁锥外时，玩家就可以认为自己是安全的。

导弹有未来预测曲线，只有被追击玩家能看到，该曲线由导弹的制导率和被追击玩家的当前舵量运动预测曲线计算得到，可以看到当前舵量下导弹在未来什么时候能减速至无威胁（导弹的预测曲线从该时空位置开始变成白色）。同样采取0.5秒更新一次，使用大微分步长。

预计加入时空图以后，不仅人类玩家的游玩体验会大大改进，而且RL agent的训练效果也能提升。

时空图的计算部分和可视化部分要分离，计算部分要保证对RL批量训练的友好性。

三、总结

人类玩家看到的时空图未来部分包含三个部分：敌方的威胁锥、我方的三条轨迹预测线、追击自己的导弹的预测线，都是围绕自身的防御而设置的时空图，其中全力掉头线可以手动开关，人类玩家如果想要看自己的进攻时空图，可以选择看对方的屏幕。

AI玩家可以看到双方全部的时空图，并且是稀疏采样点简化的。对观察空间维度较小的模型，时空图还可以提供战斗环境当中关于导弹飞行过程预测的低维特征，比如说攻击包线与全力脱离线最近距离，导弹预测线与当前舵量线最近距离（等于0表示命中）。


/Beta/：冷启动特性描述：

- 空战对抗本质上是多智能体之间的博弈，而非单智能体与静态环境的交互。如果在完全随机初始化的前提下，一开始就让两个水平都很低的智能体互相对战，即便训练极长时间，也很难从高噪声、低质量的对局中学到有意义的策略。
- 为了解决这个“冷启动”问题，在进入正式的多智能体自对弈训练之前，需要采用难度循序渐进的课程式训练：先用设计好的规则式对手和有针对性的奖励函数，引导被训智能体先掌握基础战术，再逐步过渡到复杂对抗。
- 当前设计的冷启动课程分为四个阶段，对应到训练脚本中通过 --mode cold_start 启动，并由 run_curriculum 函数按阶段顺序自动执行：
- 第一阶段：学习对准敌人、抵近敌人并节约弹药。场景中敌机初始位于本机后方，被训智能体需要以尽可能少的弹药将其击落，对应课程中的 Phase 1（scenario= phase1 ，reward= phase1 ）。
- 第二阶段：学习面对简单导弹威胁时的机动和躲避。对手为脚本化的敌机：先朝本机飞行，在最大射程附近（Rmax ≈ 25km）发射导弹，继续直飞若干秒后掉头逃离，对应 Phase 2（scenario= standard ，opponent= phase2_opponent ，reward= phase2 ）。
- 第三阶段：学习对抗会躲导弹、并尝试反攻的对手脚本。该阶段的规则式对手既会做防御机动，也会寻找机会重新发起攻击，对应 Phase 3（scenario= standard ，opponent= phase3_opponent ，reward= phase3 ）。
- 第四阶段：在完成前三个阶段后，切换为与自身历史版本进行对抗训练（self-play）。此时对手由前一阶段保存的 checkpoint 加载，作为静态的“旧版本自我”，对应 Phase 4（opponent_type= self ，reward= standard ）。
- 通过上述分阶段的冷启动课程，训练流程从“对简单脚本学基础技能”平滑过渡到“与自身旧版本对抗的自博弈训练”，既提高了早期样本的有效性，又减少了直接随机自对弈带来的训练不稳定性和收敛困难。


/Gamma/：高速并行环境
采用Nvidia Warp和CUDA Graph Capture实现高并行高速物理帧更新，在1050显卡上能让一万个环境跑出5000到10000的SPS，当SPS为6000时时间加速倍率为100倍。